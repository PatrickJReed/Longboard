{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import glob, os, gc, sys\n",
    "import os.path\n",
    "import csv\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from time import time\n",
    "from subprocess import (call, Popen, PIPE)\n",
    "from itertools import product\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from IPython.display import Image as IPImage\n",
    "import shutil\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten,Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Lambda, concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, CSVLogger, EarlyStopping, TensorBoard\n",
    "from keras import Model\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.models import load_model\n",
    "import uuid\n",
    "import pickle\n",
    "from boto3.session import Session\n",
    "import boto3\n",
    "import h5py\n",
    "\n",
    "\n",
    "##Path to Data\n",
    "basepath = \"/home/ubuntu/\"\n",
    "ACCESS_KEY = 'AKIAJNNOA6QMT7HXF6GA'\n",
    "SECRET_KEY = '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'\n",
    "session = Session(aws_access_key_id=ACCESS_KEY,aws_secret_access_key=SECRET_KEY)\n",
    "s3 = session.resource('s3')\n",
    "#Subjects = [\"USD22\", \"USD01\", \"USD11\",\"USD25\",\"USD30\",\"USD37\",\"USH12\",\"USD3\",\"USH11\",\"USD41\"]\n",
    "Subjects = [\"USD11\"] #sys.argv[1]  #subjectid\n",
    "Cells = [\"USD11A1_S105\",\"USD11A2_S169\",\"USD11A3_S97\",\"USD11A4_S161\",\"USD11A5_S113\",\"USD11A6_S121\",\"USD11B1_S106\",\"USD11B2_S170\",\"USD11B3_S98\",\"USD11B4_S162\",\"USD11B5_S114\",\"USD11B6_S122\",\"USD11C1_S107\",\"USD11C2_S171\",\"USD11C3_S99\",\"USD11C4_S163\",\"USD11C5_S115\",\"USD11C6_S123\",\"USD11D1_S108\",\"USD11D2_S172\",\"USD11D3_S100\",\"USD11D4_S164\",\"USD11D5_S116\",\"USD11D6_S124\",\"USD11E1_S109\",\"USD11E2_S173\",\"USD11E3_S101\",\"USD11E4_S165\",\"USD11E5_S117\",\"USD11E6_S125\",\"USD11F1_S110\",\"USD11F2_S174\",\"USD11F3_S102\",\"USD11F4_S166\",\"USD11F5_S118\",\"USD11F6_S126\",\"USD11G1_S111\",\"USD11G2_S175\",\"USD11G3_S103\",\"USD11G4_S167\",\"USD11G5_S119\",\"USD11G6_S127\",\"USD11H1_S112\",\"USD11H2_S176\",\"USD11H3_S104\",\"USD11H4_S168\",\"USD11H5_S120\",\"USD11H6_S128\"]\n",
    "\n",
    "img_width, img_height = 512,512\n",
    "nb_epochs = 10\n",
    "batch_size = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download training_data_all\n",
    "#s3.meta.client.download_file('bsmn-data',os.path.join('Training.h5'),os.path.join('Training.h5'))\n",
    "\n",
    "hf = h5py.File(os.path.join('Training.h5'), 'r')\n",
    "Train_Y = hf['Y'][()]\n",
    "Train_labels = hf['HDB'][()]\n",
    "Classes = len(set(Train_labels))\n",
    "\n",
    "#pull all cells per sample from Train_Y\n",
    "L = len(Train_labels)\n",
    "T={}\n",
    "\n",
    "#cell_id = Train_Y[i].strip().split('-')[0]\n",
    "for i in range(0, L):\n",
    "    position_key = Train_Y[i]\n",
    "    Y_Class = Train_labels[i]\n",
    "    T[position_key] = Y_Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0818 19:50:54.911581 140192267306752 deprecation_wrapper.py:119] From /home/ubuntu/miniconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0818 19:50:54.928782 140192267306752 deprecation_wrapper.py:119] From /home/ubuntu/miniconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0818 19:50:54.932583 140192267306752 deprecation_wrapper.py:119] From /home/ubuntu/miniconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0818 19:50:54.953627 140192267306752 deprecation_wrapper.py:119] From /home/ubuntu/miniconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0818 19:50:54.955044 140192267306752 deprecation_wrapper.py:119] From /home/ubuntu/miniconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0818 19:50:54.990966 140192267306752 deprecation_wrapper.py:119] From /home/ubuntu/miniconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0818 19:50:55.303318 140192267306752 deprecation_wrapper.py:119] From /home/ubuntu/miniconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0818 19:50:55.926650 140192267306752 deprecation_wrapper.py:119] From /home/ubuntu/miniconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "W0818 19:51:03.447513 140192267306752 deprecation.py:506] From /home/ubuntu/miniconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0818 19:51:03.486399 140192267306752 deprecation_wrapper.py:119] From /home/ubuntu/miniconda2/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0818 19:51:03.509488 140192267306752 deprecation.py:323] From /home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py:180: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#TRAIN inception model on SLAV\n",
    "train_datagen = ImageDataGenerator(width_shift_range=0.2,zoom_range=0.1,horizontal_flip=True)\n",
    "validate_datagen = ImageDataGenerator(horizontal_flip=True)\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "base_model = InceptionV3(input_shape=input_shape, weights=None, include_top=False)\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dropout(.2)(x)\n",
    "predictions = Dense(Classes, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "logs_base_dir = \"./logs\"\n",
    "checkpoint = ModelCheckpoint(\"Longboard_model.h5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='loss', patience=10, verbose=1, mode='auto')\n",
    "csv_logger = CSVLogger('training_longboard.log', append=True, separator=';')\n",
    "#tensorboard = TensorBoard(log_dir=logs_base_dir, histogram_freq=0,\n",
    "                          #write_graph=True, write_images=False)\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir {logs_base_dir}\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USD11\n",
      "USD11A1_S105\n",
      "(3280, 512, 512, 3)\n",
      "(3280, 94)\n",
      "Epoch 1/1\n",
      "218/218 [==============================] - 1580s 7s/step - loss: 0.0532 - acc: 0.9892 - val_loss: 0.1246 - val_acc: 0.9851\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.05322, saving model to Longboard_model.h5\n",
      "USD11A2_S169\n"
     ]
    }
   ],
   "source": [
    "for e in range(nb_epochs):    \n",
    "    for subject in Subjects:\n",
    "        print(subject)\n",
    "        for cell in Cells:\n",
    "            print(cell)\n",
    "            count=0\n",
    "            cell_ids = []\n",
    "            s3.meta.client.download_file('bsmn-data',os.path.join(subject, cell+'_IDs.h5'),os.path.join(basepath,cell+'_IDs.h5'))\n",
    "            f = h5py.File(os.path.join(basepath,cell+'_IDs.h5'), 'r')\n",
    "            cell_ids = f['ID']\n",
    "            for cid in cell_ids:\n",
    "                s3.meta.client.download_file('bsmn-data',os.path.join(subject, cell+'_'+cid+'.h5'),os.path.join(basepath,cell+'_'+cid+'.h5'))\n",
    "                xyz = h5py.File(os.path.join(basepath,cell+'_'+cid+'.h5'), 'r')\n",
    "                os.remove(os.path.join(basepath,cell+'_'+cid+'.h5'))\n",
    "                if count == 0:\n",
    "                    X = xyz['X'][()]\n",
    "                    Y = xyz['Y'][()]\n",
    "                    count+=1\n",
    "                else:\n",
    "                    X = np.append(X,xyz['X'][()], axis=0)\n",
    "                    Y = np.append(Y,xyz['Y'][()], axis=0)\n",
    "            Labels = [None] * len(Y)\n",
    "            for i in range(0,len(Y)):\n",
    "                Labels[i] = T[Y[i]]\n",
    "            label_encoder = LabelEncoder()\n",
    "            integer_encoded = label_encoder.fit_transform(Labels)\n",
    "            onehot_encoder = OneHotEncoder(sparse=False)\n",
    "            integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "            onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "            print(X.shape)\n",
    "            print(onehot_encoded.shape)\n",
    "            X_train, X_validate, Y_train, Y_validate = train_test_split(X, onehot_encoded, test_size=0.2, random_state=42)\n",
    "            X = None\n",
    "            Y = None\n",
    "            model.fit_generator(train_datagen.flow(X_train, Y_train, batch_size=batch_size), callbacks = [early,checkpoint,csv_logger], verbose = 1, epochs=1, steps_per_epoch=len(X_train) / 12, validation_data=validate_datagen.flow(X_validate, Y_validate, batch_size=batch_size), validation_steps=len(X_validate) / 12)\n",
    "            model.save('Longboard_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
