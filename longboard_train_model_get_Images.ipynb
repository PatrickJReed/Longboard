{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import glob, os, gc, sys\n",
    "import os.path\n",
    "import csv\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from time import time\n",
    "from subprocess import (call, Popen, PIPE)\n",
    "from itertools import product\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from IPython.display import Image as IPImage\n",
    "import shutil\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten,Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Lambda, concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, CSVLogger, EarlyStopping, TensorBoard\n",
    "from keras import Model\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.models import load_model\n",
    "import uuid\n",
    "import pickle\n",
    "from boto3.session import Session\n",
    "import boto3\n",
    "import h5py\n",
    "\n",
    "\n",
    "##Path to Data\n",
    "basepath = \"/home/ubuntu/\"\n",
    "ACCESS_KEY = 'AKIAJNNOA6QMT7HXF6GA'\n",
    "SECRET_KEY = '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'\n",
    "session = Session(aws_access_key_id=ACCESS_KEY,aws_secret_access_key=SECRET_KEY)\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "img_width, img_height = 512,512\n",
    "nb_epochs = 500\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 184763 images belonging to 4 classes.\n",
      "Found 90999 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "K.get_session().run(tf.global_variables_initializer())\n",
    "#TRAIN inception model on SLAV\n",
    "data_generator = ImageDataGenerator(rescale=1./255, validation_split=0.33)\n",
    "train_generator = data_generator.flow_from_directory(os.path.join(basepath,\"Images\"), shuffle=True, seed=13, class_mode='categorical', batch_size=batch_size, subset=\"training\", target_size=(img_width, img_height))\n",
    "validation_generator = data_generator.flow_from_directory(os.path.join(basepath,\"Images\"), shuffle=True, seed=13, class_mode='categorical', batch_size=batch_size, subset=\"validation\", target_size=(img_width, img_height))\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "base_model = InceptionV3(input_shape=input_shape, weights=None, include_top=False)\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "#x = Dropout(.2)(x)\n",
    "predictions = Dense(4, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "logs_base_dir = \"./logs\"\n",
    "checkpoint = ModelCheckpoint(\"Longboard_model.h5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='loss', patience=10, verbose=1, mode='auto')\n",
    "csv_logger = CSVLogger('training_longboard.log', append=True, separator=';')\n",
    "#tensorboard = TensorBoard(log_dir=logs_base_dir, histogram_freq=0,\n",
    "                          #write_graph=True, write_images=False)\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir {logs_base_dir}\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weight_list = compute_class_weight('balanced', np.unique(train_generator.classes), train_generator.classes)\n",
    "class_weight = dict(zip(np.unique(train_generator.classes), class_weight_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.3168002935447587,\n",
       " 1: 8.059806316524167,\n",
       " 2: 2.40827685088634,\n",
       " 3: 3.288065916856492}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " 649/5773 [==>...........................] - ETA: 48:50 - loss: 1.0918 - acc: 0.3726"
     ]
    }
   ],
   "source": [
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=nb_epochs,\n",
    "                    callbacks = [early,checkpoint,csv_logger], \n",
    "                    verbose = 1,\n",
    "                    class_weight=class_weight)\n",
    "model.save('Longboard_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xzy=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xyz=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.get_session().run(tf.global_variables_initializer())\n",
    "for e in range(nb_epochs):    \n",
    "    for subject in Subjects:\n",
    "        print(subject)\n",
    "        for cell in Cells:\n",
    "            count=0\n",
    "            print(cell)\n",
    "            cell_ids = []\n",
    "            s3.meta.client.download_file('bsmn-data',os.path.join(subject, cell+'_IDs.h5'),os.path.join(basepath,cell+'_IDs.h5'))\n",
    "            f = h5py.File(os.path.join(basepath,cell+'_IDs.h5'), 'r')\n",
    "            os.remove(os.path.join(basepath,cell+'_IDs.h5'))\n",
    "            cell_ids = f['ID']\n",
    "            for cid in cell_ids:\n",
    "                s3.meta.client.download_file('bsmn-data',os.path.join(subject, cell+'_'+cid+'.h5'),os.path.join(basepath,cell+'_'+cid+'.h5'))\n",
    "                xyz = h5py.File(os.path.join(basepath,cell+'_'+cid+'.h5'), 'r')\n",
    "                os.remove(os.path.join(basepath,cell+'_'+cid+'.h5'))\n",
    "                if count == 0:\n",
    "                    X = xyz['X'][()]\n",
    "                    Y = xyz['Y'][()]\n",
    "                    count+=1\n",
    "                else:\n",
    "                    X = np.append(X,xyz['X'][()], axis=0)\n",
    "                    Y = np.append(Y,xyz['Y'][()], axis=0)\n",
    "            print(X.shape)\n",
    "            Labels = [None] * len(Y)\n",
    "            for i in range(0,len(Y)):\n",
    "                Labels[i] = T[Y[i]]\n",
    "            rm=[]\n",
    "            for i in range(0,len(Labels)):               \n",
    "                if Labels[i] == '-1': \n",
    "                    rm=np.append(rm,i)\n",
    "            X = np.delete(X,rm,0)\n",
    "            Labels = np.delete(Labels,rm,0)\n",
    "            label_encoder = LabelEncoder()\n",
    "            integer_encoded = label_encoder.fit_transform(Labels)\n",
    "            onehot_encoder = OneHotEncoder(sparse=False)\n",
    "            integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "            onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "            print(X.shape)\n",
    "            print(onehot_encoded.shape)\n",
    "            Class_Count=[None] * int(Classes-1)\n",
    "            for i in range(Classes-1):\n",
    "                index = np.where(integer_encoded == i)\n",
    "                Class_Count[i] = len(index[0])\n",
    "            total = float(sum(Class_Count))\n",
    "            Class_Weight = [1/float(float(Class_Count[i]) / float(total)) for i in range(len(Class_Count))]\n",
    "            class_weights = {i : 1-Class_Weight[i] for i in range(Classes-1)}      \n",
    "            X_train, X_validate, Y_train, Y_validate = train_test_split(X, onehot_encoded, test_size=0.2, random_state=42)\n",
    "            X = None\n",
    "            Y = None\n",
    "            model.fit_generator(train_datagen.flow(X_train, Y_train, batch_size=batch_size), callbacks = [early,checkpoint,csv_logger], verbose = 1, epochs=5, class_weight=class_weights, steps_per_epoch=len(X_train) / 32, validation_data=validate_datagen.flow(X_validate, Y_validate, batch_size=batch_size), validation_steps=len(X_validate) / 32)\n",
    "            model.save('Longboard_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.15\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/miniconda2/bin/python'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
